[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Diabetes is a significant chronic medical condition which is becoming more and more prevalent in the United States. When people develop diabetes, they cannot regulate their blood glucose normally, which leads to reduced quality of life. When diabetes is not controlled, through diet, exercise, and/or medication therapy it can lead to additional medical problems. The main mechanism by which diabetes occurs, is that in a normal body, insulin is used to break down sugars from food into energy. In patients with diabetes, insulin is either not produced or does not work as normal leading to increase blood glucose. There are 2 types of diabetes.\n\nType 1 is normally genetic and the body does not produce insulin\nType 2 is normally developed throughout life and while the body may produce insulin, it does not work as it should\n\nAdditionally, diabetes can lead to cardiovascular and kidney complications. Also, patients are at risk for poor eyesight and wound healing.\nDiabetes continues to increase, in the US, with upwards of 88 million with pre-diabetes and 34 million people with diabetes. Type 2 diabetes is really the version of concern, since its prevalence varies by age, education level, income status, location, race, and many other Social Drivers of Health (SDOH)."
  },
  {
    "objectID": "EDA.html#introduction-to-diabetes",
    "href": "EDA.html#introduction-to-diabetes",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Diabetes is a significant chronic medical condition which is becoming more and more prevalent in the United States. When people develop diabetes, they cannot regulate their blood glucose normally, which leads to reduced quality of life. When diabetes is not controlled, through diet, exercise, and/or medication therapy it can lead to additional medical problems. The main mechanism by which diabetes occurs, is that in a normal body, insulin is used to break down sugars from food into energy. In patients with diabetes, insulin is either not produced or does not work as normal leading to increase blood glucose. There are 2 types of diabetes.\n\nType 1 is normally genetic and the body does not produce insulin\nType 2 is normally developed throughout life and while the body may produce insulin, it does not work as it should\n\nAdditionally, diabetes can lead to cardiovascular and kidney complications. Also, patients are at risk for poor eyesight and wound healing.\nDiabetes continues to increase, in the US, with upwards of 88 million with pre-diabetes and 34 million people with diabetes. Type 2 diabetes is really the version of concern, since its prevalence varies by age, education level, income status, location, race, and many other Social Drivers of Health (SDOH)."
  },
  {
    "objectID": "EDA.html#data-of-concern",
    "href": "EDA.html#data-of-concern",
    "title": "Exploratory Data Analysis",
    "section": "Data of Concern",
    "text": "Data of Concern\nThe data explored through this analysis comes from the Behavioral Risk Factor Surveilance System (BRFSS); a telephone survey collected through the CDC annually. It has been conducted every year since 1984 and targets over 400,000 Americans. This data we will focus on is from the 2015 survey of 441,455 individuals with 330 feature.\nThe specific dataset to explore is titled diabetes_binary_health_indicators_BRFSS2015.csv and more information can be found here.\nThis specific set is of 253,680 responses. Here, the target variable is Diabetes_binary where 0 indicates no diabetes and 1 indicates the presence of diabetes or prediabetes. Overall, there are 21 variables."
  },
  {
    "objectID": "EDA.html#variables-to-explore-through-exploratory-data-analysis-eda-modeling",
    "href": "EDA.html#variables-to-explore-through-exploratory-data-analysis-eda-modeling",
    "title": "Exploratory Data Analysis",
    "section": "Variables to Explore Through Exploratory Data Analysis (EDA) & Modeling",
    "text": "Variables to Explore Through Exploratory Data Analysis (EDA) & Modeling\nFor this analysis and modeling, we are going to limit our scope and only explore 4 predictor variables.\n\nTarget Variable:\n\nDiabetes_binary\n\n0 = No Diabetes\n1 = Diabetes or Prediabetes\n\n\n\n\nPredictor Variables:\n\nPhysActivity: Did the subject have physical activity within the past 30 days (excluding job activities)\n\n0 = No\n1 = Yes\n\nVeggies: Did the subject consume 1 or more vegetables in the past 30 days\n\n0 = No\n1 = Yes\n\nHvyAlcoholConsump: Did the subject consume ≥ 14 (male) or ≥ 7 (female) drinks per week\n\n0 = No\n1 = Yes\n\nBMI: Body Mass Index (will later create as a factor and breakdown the categories based on the CDC definition below)\n\n&lt; 18.5 kg/m^2 = Underweight\n18.5 to &lt; 25 kg/m^2 = Healthy Weight\n25 to &lt; 30 kg/m^2 = Overweight\n30 to &lt; 35 kg/m^2 = Class 1 Obesity\n35 to 40 kg/m^2 = Class 2 Obesity\n&gt; 40 kg/m^2 = Severe Obesity\n\n\n\n\nChoice of Variables:\nFor this model, 4 predictor variables weer chosen. As a pharmacist, I know how the affect of some of these variables would affect someone’s development of prediabetes and diabetes. Diet and exercise are some of the most important factors that a patient has direct control of, which can either lower or increase their risk of development of diabetes. As a result, I chose whether or not the subject has physical activity to indicate whether or not they live a sedentary lifestyle. Then I decided on their vegetable and alcohol consumption as addition variables to consider. If a patient increased their vegetable intake and / or decreased alcohol intake, they likely can reduce their risk of diabetes. Finally, I did choose BMI as a 4th variable. This is another factor a patient has control over and can lead to diabetes if they have higher BMIs. I also wanted at least 1 variable to be more than just binary, so BMI will be later converted to categories.\nIdeally, these variables would have had more quantitative criteria, but the survey was very broad. For example, it would’ve been more ideal to ask for how much exercise per week, not just if you had physical activity 1 time in a month. As a result, this model may not be a great predictor of someone having prediabetes or diabetes."
  },
  {
    "objectID": "EDA.html#purpose-of-this-eda-and-model",
    "href": "EDA.html#purpose-of-this-eda-and-model",
    "title": "Exploratory Data Analysis",
    "section": "Purpose of this EDA and Model",
    "text": "Purpose of this EDA and Model\nThe purpose of this EDA is to explore some patients variables, through a survey, which may be associated with someone’s development of prediabetes or diabetes. We want to examine our chosen variables to see if maybe visualize some relationships between the variables. Before modeling our data, we can also visualize some summary statistics to understand the composition of our subjects from this survey.\nAfter, we would then want to evaluate some different models, to identify the best one that can be used to predict if someone has prediabetes or diabetes. Different types of models will be evaluated since we want to create one that is accurate and is able to perform well on new data we provide, for example a different year of the survey. Ideally, a fit model would have lower error metrics and high accuracy."
  },
  {
    "objectID": "EDA.html#data-importclean-up",
    "href": "EDA.html#data-importclean-up",
    "title": "Exploratory Data Analysis",
    "section": "Data Import/Clean Up",
    "text": "Data Import/Clean Up\nWill first put some code for our needed packages:\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nLet’s import the data:\n\ndiabetes_data_df &lt;- read.csv(\"~/ST558 Repo/Final Project/diabetes_binary_health_indicators_BRFSS2015.csv\", header = TRUE)\ndiabetes_data &lt;- as_tibble(diabetes_data_df) #convert to a tibble, likely needed later\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nNow that the data has been imported, lets look at the structure of the data and then check if there are any missing values.\n\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nGreat, it looks like there are no missing values. Unfortunately all of the variables are number, but really the majority are binary. Since we are only going to evaluate a few of the variables from the survey, the next step will be to select out those variables and do some manipulation on them alone.\n\ndiabetes_data_selected &lt;- diabetes_data |&gt; \n  select(Diabetes_binary, BMI, Veggies, PhysActivity, HvyAlcoholConsump)\n\nstr(diabetes_data_selected)\n\ntibble [253,680 × 5] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ BMI              : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Veggies          : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ PhysActivity     : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ HvyAlcoholConsump: num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nNow to make the data more readable, we will convert the variables to factors. The meaning of the 0 and 1’s were described above. Additionally, the categories for BMI are above.\n\ndiabetes_summarize &lt;- diabetes_data_selected |&gt; \n  #create factors of the binary variables\n  mutate(diabetes = factor(Diabetes_binary, levels = c(0,1), #create factors of the binary variables\n                             labels = c(\"No\", \"Yes\")),\n         veggies = factor(Veggies, levels = c(0,1), \n                          labels = c(\"No\", \"Yes\")),\n         exercise = factor(PhysActivity, levels = c(0,1),\n                           labels = c(\"No\", \"Yes\")),\n         alcohol_use = factor(HvyAlcoholConsump, levels = c(0,1),\n                        labels = c(\"No\", \"Yes\"))) |&gt; \n  #creating a factor for BMI, need to figure make categories then factor so will do separately for ease of code\n  #want factor so they are ordered appropriate during analysis\n  mutate(BMI_category = case_when(\n    BMI &lt; 18.5 ~ \"Underweight\",\n    BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Healthy Weight\",\n    BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n    BMI &gt;= 30 & BMI &lt; 35 ~ \"Class 1 Obesity\",\n    BMI &gt;= 35 & BMI &lt;=40 ~ \"Class 2 Obesity\",\n    BMI &gt; 40 ~ \"Severe Obesity\")) |&gt; \n  mutate(bmi = factor(BMI_category, levels = c(\n      \"Underweight\",\n      \"Healthy Weight\",\n      \"Overweight\",\n      \"Class 1 Obesity\",\n      \"Class 2 Obesity\",\n      \"Severe Obesity\"\n    ), ordered = TRUE)\n  ) |&gt; \n  select(diabetes, veggies, exercise, alcohol_use, bmi)\n            \n\nstr(diabetes_summarize)\n\ntibble [253,680 × 5] (S3: tbl_df/tbl/data.frame)\n $ diabetes   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ veggies    : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ exercise   : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ alcohol_use: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ bmi        : Ord.factor w/ 6 levels \"Underweight\"&lt;..: 5 3 3 3 2 3 4 3 4 2 ...\n\ndiabetes_summarize\n\n# A tibble: 253,680 × 5\n   diabetes veggies exercise alcohol_use bmi            \n   &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;       &lt;ord&gt;          \n 1 No       Yes     No       No          Class 2 Obesity\n 2 No       No      Yes      No          Overweight     \n 3 No       No      No       No          Overweight     \n 4 No       Yes     Yes      No          Overweight     \n 5 No       Yes     Yes      No          Healthy Weight \n 6 No       Yes     Yes      No          Overweight     \n 7 No       No      No       No          Class 1 Obesity\n 8 No       Yes     Yes      No          Overweight     \n 9 Yes      Yes     No       No          Class 1 Obesity\n10 No       Yes     No       No          Healthy Weight \n# ℹ 253,670 more rows"
  },
  {
    "objectID": "EDA.html#summary-of-the-date",
    "href": "EDA.html#summary-of-the-date",
    "title": "Exploratory Data Analysis",
    "section": "Summary of the Date",
    "text": "Summary of the Date\nLet’s first just do some initial counting.\n\nPresence or Absence of Diabetes\n\ndiabetes_summarize |&gt; \n  count(diabetes) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\n# A tibble: 2 × 3\n  diabetes      n percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 No       218334    86.1\n2 Yes       35346    13.9\n\n\nThis breakdown actually surprises me. I thought the presence of prediabetes or diabetes in this survey would’ve been higher, though this survey was from about 10 years ago, so that might be why.\n\n\nDoes the subject eat vegetables?\n\ndiabetes_summarize |&gt; \n  count(veggies) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\n# A tibble: 2 × 3\n  veggies      n percent\n  &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;\n1 No       47839    18.9\n2 Yes     205841    81.1\n\n\nThis is not necessarily surprising that the majority eat vegetables at least 1 time per day. Based on the information provided, it does not quantify a certain amount, just 1 time.\n\n\nDoes the subject perform physical activity?\n\ndiabetes_summarize |&gt; \n  count(exercise) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\n# A tibble: 2 × 3\n  exercise      n percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 No        61760    24.4\n2 Yes      191920    75.6\n\n\nThis is physical activity within 30 days. Similar to vegetables, it does not quantify, so these numbers make sense. I imagine if it asked more than 5 - 10 times a month, or at least 150 minutes per week, then the percent “Yes” would likely decline.\n\n\nPresence of Excessive Alcohol Use\n\ndiabetes_summarize |&gt; \n  count(alcohol_use) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\n# A tibble: 2 × 3\n  alcohol_use      n percent\n  &lt;fct&gt;        &lt;int&gt;   &lt;dbl&gt;\n1 No          239424   94.4 \n2 Yes          14256    5.62\n\n\nThis amount who drink excessively is not surprising. This variable actually quantifies things better than the previous 2, since its criteria is more specific.\n\n\nSummary of BMI Categories\n\ndiabetes_summarize |&gt; \n  count(bmi) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\n# A tibble: 6 × 3\n  bmi                 n percent\n  &lt;ord&gt;           &lt;int&gt;   &lt;dbl&gt;\n1 Underweight      3127    1.23\n2 Healthy Weight  68953   27.2 \n3 Overweight      93749   37.0 \n4 Class 1 Obesity 53451   21.1 \n5 Class 2 Obesity 22921    9.04\n6 Severe Obesity  11479    4.52\n\n\n\nggplot(diabetes_summarize, aes(bmi, fill = bmi))+\n         geom_bar() +\n  labs(x = \"BMI Category\",\n       y = \"Count\",\n       title = \"Frequency of BMI by Category\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis is a nice distribution of BMI categories. The lower level of diabetes in the overall population would tell me that overall the BMIs should rather normal (Healthy Weight/Overweight). I think the overweight range of &gt; 25 to 30 kg/m^2 BMI is not the greatest label set by the CDC.\n\n\nBar Graphs to Visual Proportions of Some Categorizations\nNow that we have looked at each variable separately let’s look at some frequencies of people with and without diabetes in some of the categories. For this one, let’s look at diabetes frequency based on physical activity, heavy alcohol use, and BMI cateogory. Will exclude vegetable consumption here since a large majority of subjects consume them.\n\ndiabetes_exercise &lt;- diabetes_summarize |&gt; \n  count(diabetes, exercise) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\ndiabetes_exercise\n\n# A tibble: 4 × 4\n  diabetes exercise      n percent\n  &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 No       No        48701   19.2 \n2 No       Yes      169633   66.9 \n3 Yes      No        13059    5.15\n4 Yes      Yes       22287    8.79\n\nggplot(diabetes_exercise, aes(x = diabetes, y = percent, fill = exercise)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(aes(label = paste0(percent, \"%\")),\n            position = position_stack(vjust = 0.5),\n            vjust = -0.3, size = 4) +\n  labs(\n    x = \"Diabetes\",\n    y = \"Percentage\",\n    title = \"Physical Activity by Diabetes\",\n    fill = \"Physical Activity within 30 Days\"\n  ) +\n  scale_fill_manual(values = c(\"No\" = \"green\", \"Yes\" = \"yellow\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI think looking at this graph, I am not sure this helps to figure out whether physical activity is associated with the absence of diabetes. In patients without, it is clear a large majority have physical activity within the past 30 days. When looking at subjects with diabetes, more did have physical activity then did not. As stated before, since this is not really quantifed as more than just physical activity within 30 days, it may not be as helpful as something that quantifies better.\n\ndiabetes_alcohol_use &lt;- diabetes_summarize |&gt; \n  count(diabetes, alcohol_use) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\ndiabetes_alcohol_use\n\n# A tibble: 4 × 4\n  diabetes alcohol_use      n percent\n  &lt;fct&gt;    &lt;fct&gt;        &lt;int&gt;   &lt;dbl&gt;\n1 No       No          204910   80.8 \n2 No       Yes          13424    5.29\n3 Yes      No           34514   13.6 \n4 Yes      Yes            832    0.33\n\nggplot(diabetes_alcohol_use, aes(x = diabetes, y = percent, fill = alcohol_use)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(aes(label = paste0(percent, \"%\")),\n            position = position_stack(vjust = 0.5),\n            vjust = -0.3, size = 4) +\n  labs(\n    x = \"Diabetes\",\n    y = \"Percentage\",\n    title = \"Alcohol Use by Presence or Absence of Diabetes\",\n    fill = \"Heavy Alcohol Use\"\n  ) +\n  scale_fill_manual(values = c(\"No\" = \"orange\", \"Yes\" = \"blue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSince there was minimal heavy alcohol use within this population, this graph may not tell us much. There is overall a small percentage of heavy alcohol use in the diabetes group.\n\ndiabetes_bmi_cat &lt;- diabetes_summarize |&gt; \n  count(diabetes, bmi) |&gt; \n  mutate(percent = round(n / sum(n) * 100, 2))\n\ndiabetes_bmi_cat\n\n# A tibble: 12 × 4\n   diabetes bmi                 n percent\n   &lt;fct&gt;    &lt;ord&gt;           &lt;int&gt;   &lt;dbl&gt;\n 1 No       Underweight      2958    1.17\n 2 No       Healthy Weight  65025   25.6 \n 3 No       Overweight      83057   32.7 \n 4 No       Class 1 Obesity 43170   17.0 \n 5 No       Class 2 Obesity 16528    6.52\n 6 No       Severe Obesity   7596    2.99\n 7 Yes      Underweight       169    0.07\n 8 Yes      Healthy Weight   3928    1.55\n 9 Yes      Overweight      10692    4.21\n10 Yes      Class 1 Obesity 10281    4.05\n11 Yes      Class 2 Obesity  6393    2.52\n12 Yes      Severe Obesity   3883    1.53\n\nggplot(diabetes_bmi_cat, aes(x = bmi, y = percent, fill = diabetes)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(aes(label = paste0(percent, \"%\")),\n            position = position_stack(vjust = 0.5),\n            vjust = -0.3, size = 4) +\n  labs(\n    x = \"BMI Category\",\n    y = \"Percentage\",\n    title = \"Diabetes Presence by BMI Category\",\n    fill = \"Diabetes Status\"\n  ) +\n  scale_fill_discrete() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs expected, when looking at the percent of subjects within each BMI category, there are higher proportions of subjects with diabetes in the higher BMI categories.\n\n\nScatter Plot to Visual Diabetes Probability based on BMI\nFinally, lets look at a scatter plot to see if we can visual the relationship between BMI and diabetes with something different than a bar graph.\n\n#need to convert BMI to numeric. Also need to take the diabetes factor and convert to numeric for\n#plotting reasons\nggplot(diabetes_summarize, aes(x = as.numeric(bmi), y = as.numeric(diabetes == \"Yes\"))) + \n  #plot a logistic line below\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  #creating our bmi categories as numeric for better plotting\n  scale_x_continuous(breaks = 1:6, labels = levels(diabetes_summarize$bmi)) +\n  labs(title = \"Probability of Diabetes by BMI Category\",\n       x = \"BMI Category\",\n       y = \"Probability of Diabetes\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis graphs help with some of our modeling ideas. Plotting a logistic line comparing the probability of having diabetes based on BMI category, as BMI increases, the likelihood of diabetes increases, which makes sense.\nClick here for the Modeling Page"
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "Modeling Diabetes",
    "section": "",
    "text": "On the EDA page, diabetes was described and we did some exploring of the data. Our ultimate goal of this project is to create some predictive models for patients who may have prediabetes or diabetes. This is based off of data from a survey conducted by the CDC yearly, but this specific data is from 2015.\nOverall, there were 21 variables, but it was decided to select only 4 to utilize as our predictors.\nThe variables are now (this is utilizing the renamed variables):\n\nexercise: Did the subject have physical activity within the past 30 days (excluding job activities)?\nveggies: Did the subject consume 1 or more vegetables in the past 30 days?\nalcohol_use: Did the subject consume ≥ 14 (male) or ≥ 7 (female) drinks per week?\nbmi: Body Mass Index (classifications below)\n\n&lt; 18.5 kg/m^2 = Underweight\n18.5 to &lt; 25 kg/m^2 = Healthy Weight\n25 to &lt; 30 kg/m^2 = Overweight\n30 to &lt; 35 kg/m^2 = Class 1 Obesity\n35 to 40 kg/m^2 = Class 2 Obesity\n&gt; 40 kg/m^2 = Severe Obesity ### Splitting our Data"
  },
  {
    "objectID": "modeling.html#introduction-to-our-model",
    "href": "modeling.html#introduction-to-our-model",
    "title": "Modeling Diabetes",
    "section": "",
    "text": "On the EDA page, diabetes was described and we did some exploring of the data. Our ultimate goal of this project is to create some predictive models for patients who may have prediabetes or diabetes. This is based off of data from a survey conducted by the CDC yearly, but this specific data is from 2015.\nOverall, there were 21 variables, but it was decided to select only 4 to utilize as our predictors.\nThe variables are now (this is utilizing the renamed variables):\n\nexercise: Did the subject have physical activity within the past 30 days (excluding job activities)?\nveggies: Did the subject consume 1 or more vegetables in the past 30 days?\nalcohol_use: Did the subject consume ≥ 14 (male) or ≥ 7 (female) drinks per week?\nbmi: Body Mass Index (classifications below)\n\n&lt; 18.5 kg/m^2 = Underweight\n18.5 to &lt; 25 kg/m^2 = Healthy Weight\n25 to &lt; 30 kg/m^2 = Overweight\n30 to &lt; 35 kg/m^2 = Class 1 Obesity\n35 to 40 kg/m^2 = Class 2 Obesity\n&gt; 40 kg/m^2 = Severe Obesity ### Splitting our Data"
  },
  {
    "objectID": "modeling.html#loading-packages",
    "href": "modeling.html#loading-packages",
    "title": "Modeling Diabetes",
    "section": "Loading Packages",
    "text": "Loading Packages\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\nsuppressPackageStartupMessages(library(caret))"
  },
  {
    "objectID": "modeling.html#setting-up-the-data-for-modeling",
    "href": "modeling.html#setting-up-the-data-for-modeling",
    "title": "Modeling Diabetes",
    "section": "Setting up the Data for Modeling",
    "text": "Setting up the Data for Modeling\nWe will use a 70/30 split of our data to create training and testing data.\n\nset.seed(145) \ndiabetes_split &lt;- initial_split(diabetes_model_data, prop = 0.7)\n\ntraining_diabetes &lt;- training(diabetes_split)\ntesting_diabetes &lt;- testing(diabetes_split)\n\nFor all models we are going to evaluate, we are going to use logLoss as our metric. To select the best model, we will also perform a 5 fold cross-validation.\n\ndiabetes_5_fold &lt;- vfold_cv(training_diabetes, 5)"
  },
  {
    "objectID": "modeling.html#logistic-regression-models",
    "href": "modeling.html#logistic-regression-models",
    "title": "Modeling Diabetes",
    "section": "Logistic Regression Models:",
    "text": "Logistic Regression Models:\n\nDefining what Logistic Regression Models Are:\nLogistic Regression Models are used when we have a binary response variable and are performing a classification task. Since we are targeting a response that is a success vs. failure (or yes vs. no), we are modeling the probability of the response given some prediction variables.\nWhen modeling the probability in the case of logistic regression, predictions would be between 0 and 1. The log-odds of the event is what is plotted during a logistic regression model.\nIn logistic regression, we are not trying to predict an actual value, but the probability of the response variable occurring.\nPredictors can be either numeric or categorical when performing a logistic regression. If using a numeric predictor, it should be centered and scaled prior to fitting the model.\n\n\nWhy should Logistic Regression Models be applied to our Diabetes Data?\nWe can apply a logistic regression model for our diabetes response variable since it is binary. The outcome is either a subject has prediabetes/diabetes or they do not. The goal of our model is to use prediction variables to identify the probability a subject will have the response variable of the disease given our chosen predictors.\n\n\nExploring our Model with Logisic Regression:\nWe will test out 3 different models and select the best 1 using logistic regression. Presence of Prediabetes/Diabetes is our target prediction.\n\nModel 1: BMI + veggies + alcohol_use + exercise\n\nThis will be a model with all variables. No interactions here. Just seeing a model with all of our chosen variables.\n\nModel 2: bmi + exercise + veggies\n\nInteraction of bmi + exercise\nInteraction of bmi + exercise + veggies\nIt is known that having healthy weight and regular exercise reduces the risk of Type 2 Diabetes. In this model, we will evaluate those 2 variables. There will also be an interaction between the 2, since it could be possible that lower BMIs may exercise more. Also, adding in if vegetables are part of the diet may just indicate overall healthier individuals. We will also model an interaction with all 3 of these variables. I wonder if all 3 would have an effect on each other and a subject’s probability of having the outcome.\n\nModel 3: veggies + alcohol_use + exercise\n\nInteraction of alcohol_use + exercise\nInteraction of alcohol_use + exercise + veggies\nThis model will again be using variables that if someone has a better diet, exercises, and does not have heavy alcohol use, they should theoretically have lower risk of diabetes. It would be interesting to see the effect of all 3. We will add in an interaction with just alcohol_use and exercise.\n\n\n\n\nModel Evaluation:\nCreating the recipes for each model below:\n\nlr_diabetes_1 &lt;- recipe(diabetes ~ ., data = training_diabetes) |&gt; \n  step_dummy(all_nominal_predictors()) #all need to be dummy, no numeric variables\n\nlr_diabetes_2 &lt;- recipe(diabetes ~ bmi + exercise + veggies, data = training_diabetes) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_interact(terms = ~ starts_with(\"bmi\"):starts_with(\"exercise\")) |&gt; \n  step_interact(terms = ~ starts_with(\"bmi\"):starts_with(\"exercise\"):starts_with(\"veggies\"))\n\nlr_diabetes_3 &lt;- recipe(diabetes ~ veggies + alcohol_use + exercise, data = training_diabetes) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_interact(terms = ~ starts_with(\"alcohol_use\"):starts_with(\"exercise\"):starts_with(\"veggies\")) |&gt; \n  step_interact(terms = ~ starts_with(\"alcohol_use\"):starts_with(\"exercise\"))\n\nlr_diabetes_1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\nlr_diabetes_2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: starts_with(\"bmi\"):starts_with(\"exercise\")\n\n\n• Interactions with:\n  starts_with(\"bmi\"):starts_with(\"exercise\"):starts_with(\"veggies\")\n\nlr_diabetes_3\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with:\n  starts_with(\"alcohol_use\"):starts_with(\"exercise\"):starts_with(\"veggies\")\n\n\n• Interactions with: starts_with(\"alcohol_use\"):starts_with(\"exercise\")\n\n\nNow let’s set up our model:\n\nlr_diabetes &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nThen create the workflow for our 3 models:\n\nlr_1_wf &lt;- workflow() |&gt; \n  add_recipe(lr_diabetes_1) |&gt; \n  add_model(lr_diabetes)\nlr_2_wf &lt;- workflow() |&gt; \n  add_recipe(lr_diabetes_2) |&gt; \n  add_model(lr_diabetes)\nlr_3_wf &lt;- workflow() |&gt; \n  add_recipe(lr_diabetes_3) |&gt; \n  add_model(lr_diabetes)\n\nFit our CV folds we created prior. We will use the metric of logLoss:\n\nlr_1_fit &lt;- lr_1_wf |&gt; \n  fit_resamples(diabetes_5_fold, metrics = metric_set(mn_log_loss))\nlr_2_fit &lt;- lr_2_wf |&gt; \n  fit_resamples(diabetes_5_fold, metrics = metric_set(mn_log_loss))\nlr_3_fit &lt;- lr_3_wf |&gt; \n  fit_resamples(diabetes_5_fold, metrics = metric_set(mn_log_loss))\n\nNext, let’s collect the metrics of our models\n\nrbind(lr_1_fit |&gt; collect_metrics(), #combine the metics from each formula\n      lr_2_fit |&gt; collect_metrics(),\n      lr_3_fit |&gt; collect_metrics()) |&gt; \n  mutate(Model = c(\"Model 1\", \"Model 2\", \"Model 3\")) |&gt; \n  select(Model, everything())\n\n# A tibble: 3 × 7\n  Model   .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 Model 1 mn_log_loss binary     0.375     5 0.000394 Preprocessor1_Model1\n2 Model 2 mn_log_loss binary     0.376     5 0.000361 Preprocessor1_Model1\n3 Model 3 mn_log_loss binary     0.396     5 0.000833 Preprocessor1_Model1\n\n\nIt looks like Model 1 is the best, but just barely. This indicates that each of these predictors veggies, alcohol_use, exercise, and bmi all independently effect whether a subject has prediabetes or diabetes. In these models, the interactions likely made the model too complex and did not increase the ability to predict, so here the simpler model won.\nHere is our final fit on the full training data:\n\nfinal_lr_diabetes &lt;- lr_1_wf |&gt; \n  fit(training_diabetes)\n\nfinal_lr_diabetes\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n        (Intercept)          veggies_Yes         exercise_Yes  \n            -2.3635              -0.2035              -0.4903  \n    alcohol_use_Yes   bmi_Healthy.Weight       bmi_Overweight  \n            -0.8557               0.1535               0.8898  \nbmi_Class.1.Obesity  bmi_Class.2.Obesity   bmi_Severe.Obesity  \n             1.4514               1.8971               2.1422  \n\nDegrees of Freedom: 177575 Total (i.e. Null);  177567 Residual\nNull Deviance:      143600 \nResidual Deviance: 133000   AIC: 133000"
  },
  {
    "objectID": "modeling.html#classification-tree",
    "href": "modeling.html#classification-tree",
    "title": "Modeling Diabetes",
    "section": "Classification Tree:",
    "text": "Classification Tree:\n\nDefining what Classification Tree Models Are:\nClassification trees are used to classify a group membership, or in other words, predict whether a binary outcome will occur. Basically the model takes the data and builds branches where the data is split and a decision is made. This occurs multiple times until we get to the final classification. The tree continues to “grow” until the maximum tree depth (that was set) is achieved. The prediction is made based on what bin an observation ends up in. Classification trees also have hyperparameters which can be tuned or set when setting up the model.\nClassification trees are easy to interpret, since the decision at each step is easy to visualize. Numeric predictors do not need to be centered or scaled. However, classification trees can become overfit by memorizing the training data and then become poor generalizers.\n\n\nWhy could we consider Classification Trees with our data?\nClassification Trees should be considered with our data since we have a binary response variable. Also, we do not need to do any prepossessing of our data, since we have categorical predictors (alone with our response).\n\n\nModel Evaluation\nNow we will fit a classification tree. First we will need to create the recipe:\n\ntree_diabetes_form &lt;- recipe(diabetes ~ ., data = training_diabetes)\n\nLet’s first define the model and engine.\n\n#tree_model &lt;- decision_tree(tree_depth = tune(), #allow for tuning of hyperparameters\n                        #    min_n = tune(),\n                        #    cost_complexity = tune()) |&gt; \n  #set_engine(\"rpart\") |&gt; \n  #set_mode(\"classification\")\ntree_model &lt;- decision_tree() |&gt;\n  set_engine(\"partykit\") |&gt;\n  set_mode(\"classification\") \n\nThen create the workflow:\n\ntree_wrkf &lt;- workflow() |&gt; \n  add_recipe(tree_diabetes_form) |&gt; \n  add_model(tree_model)\n\nNext we will use our CV folds to select the tuning parameters. We are only interested in logLoss as our metric, so also indicating that here:\n\nlog_loss_metric &lt;- metric_set(mn_log_loss)\n\ntree_grid &lt;- tree_wrkf |&gt; \n  tune_grid(\n    resamples = diabetes_5_fold,\n    metrics = log_loss_metric)\n\nWarning: No tuning parameters have been detected, performance will be evaluated\nusing the resamples with no tuning. Did you want to [tune()] parameters?\n\ntree_grid_f &lt;- tree_grid |&gt; \n  collect_metrics()\n\ntree_grid_f\n\n# A tibble: 1 × 6\n  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.374     5 0.000360 Preprocessor1_Model1\n\n\nNow that we generated some tuning parameters, let’s identify the best model.\n\ntree_best &lt;- tree_grid_f |&gt; \n  filter(.metric == \"mn_log_loss\") |&gt; \n  arrange(mean) |&gt; \n  slice(1)\n\ntree_best\n\n# A tibble: 1 × 6\n  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.374     5 0.000360 Preprocessor1_Model1\n\n\nNow we will need to finalize our workflow with the best tree:\n\nfinal_tree_wf&lt;- finalize_workflow(tree_wrkf, tree_best)\n\nfinal_tree_diabetes &lt;- final_tree_wf |&gt; \n  fit(training_diabetes)\n\nfinal_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nComputational engine: partykit \n\nfinal_tree_diabetes\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nModel formula:\n..y ~ veggies + exercise + alcohol_use + bmi\n\nFitted party:\n[1] root\n|   [2] bmi in Underweight, Healthy Weight, Overweight\n|   |   [3] bmi in Underweight, Healthy Weight\n|   |   |   [4] exercise in No\n|   |   |   |   [5] alcohol_use in No: No (n = 8499, err = 10.0%)\n|   |   |   |   [6] alcohol_use in Yes: No (n = 651, err = 3.8%)\n|   |   |   [7] exercise in Yes\n|   |   |   |   [8] veggies in No\n|   |   |   |   |   [9] alcohol_use in No: No (n = 5073, err = 7.8%)\n|   |   |   |   |   [10] alcohol_use in Yes: No (n = 297, err = 3.7%)\n|   |   |   |   [11] veggies in Yes\n|   |   |   |   |   [12] alcohol_use in No: No (n = 33360, err = 4.6%)\n|   |   |   |   |   [13] alcohol_use in Yes: No (n = 2606, err = 1.8%)\n|   |   [14] bmi in Overweight\n|   |   |   [15] exercise in No\n|   |   |   |   [16] alcohol_use in No\n|   |   |   |   |   [17] veggies in No: No (n = 3897, err = 18.6%)\n|   |   |   |   |   [18] veggies in Yes: No (n = 9520, err = 16.1%)\n|   |   |   |   [19] alcohol_use in Yes: No (n = 774, err = 9.3%)\n|   |   |   [20] exercise in Yes\n|   |   |   |   [21] alcohol_use in No\n|   |   |   |   |   [22] veggies in No: No (n = 7442, err = 13.5%)\n|   |   |   |   |   [23] veggies in Yes: No (n = 40926, err = 10.0%)\n|   |   |   |   [24] alcohol_use in Yes: No (n = 3003, err = 3.9%)\n|   [25] bmi in Class 1 Obesity, Class 2 Obesity, Severe Obesity\n|   |   [26] bmi in Class 1 Obesity\n|   |   |   [27] exercise in No\n|   |   |   |   [28] alcohol_use in No\n|   |   |   |   |   [29] veggies in No: No (n = 3002, err = 27.4%)\n|   |   |   |   |   [30] veggies in Yes: No (n = 7028, err = 24.6%)\n|   |   |   |   [31] alcohol_use in Yes: No (n = 493, err = 14.8%)\n|   |   |   [32] exercise in Yes\n|   |   |   |   [33] alcohol_use in No\n|   |   |   |   |   [34] veggies in No: No (n = 4537, err = 19.1%)\n|   |   |   |   |   [35] veggies in Yes: No (n = 21112, err = 17.0%)\n|   |   |   |   [36] alcohol_use in Yes: No (n = 1324, err = 8.5%)\n|   |   [37] bmi in Class 2 Obesity, Severe Obesity\n|   |   |   [38] exercise in No\n|   |   |   |   [39] bmi in Class 2 Obesity\n|   |   |   |   |   [40] alcohol_use in No: No (n = 5580, err = 33.3%)\n|   |   |   |   |   [41] alcohol_use in Yes: No (n = 206, err = 17.0%)\n|   |   |   |   [42] bmi in Severe Obesity\n|   |   |   |   |   [43] alcohol_use in No: No (n = 3410, err = 40.9%)\n|   |   |   |   |   [44] alcohol_use in Yes: No (n = 95, err = 20.0%)\n|   |   |   [45] exercise in Yes\n\n...\nand 9 more lines."
  },
  {
    "objectID": "modeling.html#random-forest",
    "href": "modeling.html#random-forest",
    "title": "Modeling Diabetes",
    "section": "Random Forest:",
    "text": "Random Forest:\n\nDefining what Random Forest Models Are:\nRandom forest models are ensemble learning models where multiple decision trees are combined to make a prediction. During the training of the model, many trees are built and the final prediction is based on an aggregate of all of individual trees. The final predication is based on the average prediction or majority of the classification. Random Forest models are similar to classification trees, but they normally use bootstrap aggregation where each tree is training on a random subset of data. Also, at each decision point, a random subset of variables are considered (not all variables). You also have the ability to tune for hyperparameters.\nThe advantage of random forest models over a classification tree is that there is normally higher accuracy due to the randomness of the training data. Also, there is likely less variance and reduced overfitting.\nOn the other hand, random forest models do require a high cost to compute.\n\n\nWhy could we consider Random Forests with our data?\nA Random Forest Model could be considered for our data since they can predict a binary outcome. Also, the random forest model would likely create a better model than a classification tree, due to the ensemble nature of the model.\n\n\nModel Evaluation\nNow let’s fit our model. Let’s first create a formula:\n\nrf_diabetes_form &lt;- recipe(diabetes ~ ., data = training_diabetes)\n\nLet’s first create our model:\n\nrf_model &lt;- rand_forest(mtry = tune()) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\")\n\nThen we will make our workflow:\n\nrf_wrkf &lt;- workflow() |&gt; \n  add_recipe(rf_diabetes_form) |&gt; \n  add_model(rf_model)\n\nNow fit our CV folds:\n\nrf_fit &lt;- rf_wrkf |&gt; \n  tune_grid(resamples = diabetes_5_fold,\n            grid = 7,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt; \n  collect_metrics() |&gt; \n  filter(.metric == \"mn_log_loss\") |&gt; \n  arrange(mean)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.374     5 0.000387 Preprocessor1_Model1\n2     4 mn_log_loss binary     0.375     5 0.000549 Preprocessor1_Model3\n3     2 mn_log_loss binary     0.375     5 0.000432 Preprocessor1_Model4\n4     1 mn_log_loss binary     0.381     5 0.000776 Preprocessor1_Model2\n\n\nNow select the mode with the best parameters:\n\nrf_best &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     3 Preprocessor1_Model1\n\n\nNow we will create our final workflow:\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wrkf, rf_best)\n\nfinal_rf_diabetes &lt;- final_rf_wf |&gt; fit(training_diabetes)\n\nfinal_rf_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 3\n\nComputational engine: ranger \n\nfinal_rf_diabetes\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      177576 \nNumber of independent variables:  4 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1126478"
  },
  {
    "objectID": "modeling.html#comparing-the-3-models-and-selecting-the-best-model",
    "href": "modeling.html#comparing-the-3-models-and-selecting-the-best-model",
    "title": "Modeling Diabetes",
    "section": "Comparing the 3 Models and Selecting the Best Model",
    "text": "Comparing the 3 Models and Selecting the Best Model\nFrom our logistic regression model lr_1_fit was the best. This was the simple model with all 4 predictors set an independent variables. Below are the results with the logloss of 0.3745.\n\nlr_1_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 6\n  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.375     5 0.000394 Preprocessor1_Model1\n\n\nFrom our classification tree, our best model was tree_best. The parameters are below. It looks like we have a cost complexity of 1^-10, a depth of 5, and the min_n of 18. This model produced a logloss of 0.4045.\n\ntree_best\n\n# A tibble: 1 × 6\n  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.374     5 0.000360 Preprocessor1_Model1\n\n\nAnd finally here is our best random forest model:\n\nrf_best\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     3 Preprocessor1_Model1\n\n\nNow let’s fit our 3 best models to the testing data to evaluate the best model:\nLet’s predict based on the test data:\n\npredict_lr &lt;- predict(final_lr_diabetes, #predict based on final lr model\n                      testing_diabetes,  #test data\n                      type = \"prob\") |&gt;  #goal is probabilities\n  bind_cols(testing_diabetes)\npredict_tree &lt;- predict(final_tree_diabetes, #repeat as above for tree\n                      testing_diabetes, \n                      type = \"prob\") |&gt; \n  bind_cols(testing_diabetes)\npredict_rf &lt;- predict(final_rf_diabetes, #repeat for random forest\n                    testing_diabetes, \n                    type = \"prob\") |&gt; \n  bind_cols(testing_diabetes)\n\npredict_lr\n\n# A tibble: 76,104 × 7\n   .pred_No .pred_Yes diabetes veggies exercise alcohol_use bmi            \n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;       &lt;fct&gt;          \n 1    0.877    0.123  No       No      Yes      No          Overweight     \n 2    0.814    0.186  No       No      No       No          Overweight     \n 3    0.753    0.247  Yes      Yes     No       No          Class 1 Obesity\n 4    0.918    0.0822 No       Yes     No       No          Healthy Weight \n 5    0.753    0.247  No       Yes     No       No          Class 1 Obesity\n 6    0.843    0.157  Yes      Yes     No       No          Overweight     \n 7    0.833    0.167  No       Yes     Yes      No          Class 1 Obesity\n 8    0.901    0.0989 No       No      No       No          Healthy Weight \n 9    0.843    0.157  No       Yes     No       No          Overweight     \n10    0.833    0.167  No       Yes     Yes      No          Class 1 Obesity\n# ℹ 76,094 more rows\n\npredict_tree\n\n# A tibble: 76,104 × 7\n   .pred_No .pred_Yes diabetes veggies exercise alcohol_use bmi            \n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;       &lt;fct&gt;          \n 1    0.865    0.135  No       No      Yes      No          Overweight     \n 2    0.814    0.186  No       No      No       No          Overweight     \n 3    0.754    0.246  Yes      Yes     No       No          Class 1 Obesity\n 4    0.900    0.0999 No       Yes     No       No          Healthy Weight \n 5    0.754    0.246  No       Yes     No       No          Class 1 Obesity\n 6    0.839    0.161  Yes      Yes     No       No          Overweight     \n 7    0.830    0.170  No       Yes     Yes      No          Class 1 Obesity\n 8    0.900    0.0999 No       No      No       No          Healthy Weight \n 9    0.839    0.161  No       Yes     No       No          Overweight     \n10    0.830    0.170  No       Yes     Yes      No          Class 1 Obesity\n# ℹ 76,094 more rows\n\npredict_rf\n\n# A tibble: 76,104 × 7\n   .pred_No .pred_Yes diabetes veggies exercise alcohol_use bmi            \n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;       &lt;fct&gt;          \n 1    0.868     0.132 No       No      Yes      No          Overweight     \n 2    0.817     0.183 No       No      No       No          Overweight     \n 3    0.754     0.246 Yes      Yes     No       No          Class 1 Obesity\n 4    0.900     0.100 No       Yes     No       No          Healthy Weight \n 5    0.754     0.246 No       Yes     No       No          Class 1 Obesity\n 6    0.839     0.161 Yes      Yes     No       No          Overweight     \n 7    0.829     0.171 No       Yes     Yes      No          Class 1 Obesity\n 8    0.893     0.107 No       No      No       No          Healthy Weight \n 9    0.839     0.161 No       Yes     No       No          Overweight     \n10    0.829     0.171 No       Yes     Yes      No          Class 1 Obesity\n# ℹ 76,094 more rows\n\n\nNow we need to calculate the log loss for each:\n\nlogloss_lr &lt;- mn_log_loss(predict_lr, truth = diabetes, .pred_Yes) \n  #calculating log loss\n  #above needs the prediction, then the truth (diabetes column) and \n  #.pred_Yes indicated predicted probabilities\nlogloss_tree &lt;- mn_log_loss(predict_tree, truth = diabetes, .pred_Yes)\nlogloss_rf &lt;- mn_log_loss(predict_rf, truth = diabetes, .pred_Yes)\n\nHere are the final model results:\n\nresults_diabetes_models &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(logloss_lr$.estimate, logloss_tree$.estimate, logloss_rf$.estimate)\n)\n\nprint(results_diabetes_models)\n\n# A tibble: 3 × 2\n  Model               LogLoss\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Logistic Regression    1.94\n2 Classification Tree    1.95\n3 Random Forest          1.95\n\n\nOverall, the logistic regression model performed well. I wonder if the simpler model won out, since all of our predictor variables are categorical, with 3 of them being binary. It also is likely that all of the chosen predictors do really independently affect a subject’s probability of prediabetes or diabetes.\n\n#save the model to use later for our api\n\nsaveRDS(final_lr_diabetes, \n        file = file.path(\"~\", \"ST558 Repo\", \n                         \"Final Project\", \n                         \"best_diabetes_model.rds\"))"
  },
  {
    "objectID": "modeling.html#loading-packages-and-data",
    "href": "modeling.html#loading-packages-and-data",
    "title": "Modeling Diabetes",
    "section": "Loading Packages and Data",
    "text": "Loading Packages and Data\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\nsuppressPackageStartupMessages(library(yardstick))\nsuppressPackageStartupMessages(library(tibble))\nsuppressPackageStartupMessages(library(bonsai))\nsuppressPackageStartupMessages(library(partykit))\n\ndiabetes_data_df &lt;- read.csv(\"~/ST558 Repo/Final Project/diabetes_binary_health_indicators_BRFSS2015.csv\", header = TRUE)\ndiabetes_data &lt;- as_tibble(diabetes_data_df) #convert to a tibble, likely needed later\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nWe will also manipulate data similar to the EDA for ease of modeling:\n\ndiabetes_model_data &lt;- diabetes_data |&gt; \n  select(Diabetes_binary, BMI, Veggies, PhysActivity, HvyAlcoholConsump) |&gt; \n  mutate(diabetes = factor(Diabetes_binary, levels = c(0,1), #create factors of the binary variables\n                             labels = c(\"No\", \"Yes\")),\n         veggies = factor(Veggies, levels = c(0,1), \n                          labels = c(\"No\", \"Yes\")),\n         exercise = factor(PhysActivity, levels = c(0,1),\n                           labels = c(\"No\", \"Yes\")),\n         alcohol_use = factor(HvyAlcoholConsump, levels = c(0,1),\n                        labels = c(\"No\", \"Yes\"))) |&gt; \n  #creating a factor for BMI, need to figure make categories then factor so will do separately for ease of code\n  #want factor so they are ordered appropriate during analysis\n  mutate(BMI_category = case_when(\n    BMI &lt; 18.5 ~ \"Underweight\",\n    BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Healthy Weight\",\n    BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n    BMI &gt;= 30 & BMI &lt; 35 ~ \"Class 1 Obesity\",\n    BMI &gt;= 35 & BMI &lt;=40 ~ \"Class 2 Obesity\",\n    BMI &gt; 40 ~ \"Severe Obesity\")) |&gt; \n  mutate(bmi = factor(BMI_category, levels = c(\n      \"Underweight\",\n      \"Healthy Weight\",\n      \"Overweight\",\n      \"Class 1 Obesity\",\n      \"Class 2 Obesity\",\n      \"Severe Obesity\"\n    ))\n  ) |&gt; \n  select(diabetes, veggies, exercise, alcohol_use, bmi)\n\ndiabetes_model_data\n\n# A tibble: 253,680 × 5\n   diabetes veggies exercise alcohol_use bmi            \n   &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;       &lt;fct&gt;          \n 1 No       Yes     No       No          Class 2 Obesity\n 2 No       No      Yes      No          Overweight     \n 3 No       No      No       No          Overweight     \n 4 No       Yes     Yes      No          Overweight     \n 5 No       Yes     Yes      No          Healthy Weight \n 6 No       Yes     Yes      No          Overweight     \n 7 No       No      No       No          Class 1 Obesity\n 8 No       Yes     Yes      No          Overweight     \n 9 Yes      Yes     No       No          Class 1 Obesity\n10 No       Yes     No       No          Healthy Weight \n# ℹ 253,670 more rows\n\n#write to .csv to use later\nsaveRDS(diabetes_model_data, \"~/ST558 Repo/Final Project/model_data.rds\")"
  }
]