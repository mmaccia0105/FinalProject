---
title: "Modeling Diabetes"
format: html
author: Mike Maccia
editor: visual
---

## Introduction to Our Model

On the EDA page, diabetes was described and we did some exploring of the data. Our ultimate goal of this project is to create some predictive models for patients who may have prediabetes or diabetes. This is based off of data from a survey conducted by the CDC yearly, but this specific data is from 2015.

Overall, there were 21 variables, but it was decided to select only 4 to utilize as our predictors.

The variables are now (this is utilizing the renamed variables):

-   `exercise`: Did the subject have physical activity within the past 30 days (excluding job activities)

-   `veggies`: Did the subject consume 1 or more vegetables in the past 30 days

-   `alcohol_use`: Did the subject consume ≥ 14 (male) or ≥ 7 (female) drinks per week

-   `bmi`: Body Mass Index (classifications below)

    -   \< 18.5 kg/m\^2 = Underweight

    -   18.5 to \< 25 kg/m\^2 = Healthy Weight

    -   25 to \< 30 kg/m\^2 = Overweight

    -   30 to \< 35 kg/m\^2 = Class 1 Obesity

    -   35 to 40 kg/m\^2 = Class 2 Obesity

    -   \> 40 kg/m\^2 = Severe Obesity \### Splitting our Data
    
## Loading Packages

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(caret))
```

## Setting up the Data for Modeling

We will use a 70/30 split of our data to create training and testing data. 

```{r}
set.seed(145) 
diabetes_split <- initial_split(diabetes_summarize, prop = 0.7)

training_diabetes <- training(diabetes_split)
testing_diabetes <- testing(diabetes_split)

```

For all models we are going to evaluate, we are going to use `logLoss` as our `metric`. To select the best model, we will also perform a 5 fold cross-validation. 

```{r}
cv_5 <- trainControl(method = "cv", number = 5) 
```

## Logistic Regression Models:

### Defining what Logistic Regression Models Are:

Logistic Regression Models are used when we have a binary response variable and we are performing a classification task. Since we are targeting a response that is a success vs. failure (or yes vs. no), we are modeling the probability of the response given some prediction variables. 

When modeling the probability in the case of logistic regression, predictions would be between 0 and 1. The log-odds of the event is what is plotted during a logistic regression model. 

In logistic regression, we are not trying to predict an actual value, but the probability of the response variable occurring. 

Predictors can be either numeric or categorical when performing a logistic regression. If using a numeric predictor, it should be centered and scaled prior to fitting the model. 

### Why should Logistic Regression Models be applied to our Diabetes Data?

We can apply a logistic regression model for our diabetes response variable since it is binary. The outcome is either a subject has prediabetes/diabetes or they do not. The goal of our model is to use prediction variables to identify the probability a subject will have the response variable of the disease. 

## Classification Tree:

### Defining what Classification Tree Models Are:

Classification trees are used to classify a group membership, or in other words, predict whether a binary outcome will occur. Basically the model takes the data and builds branches where the data is split and a decision is made. This occurs multiple times until we get to the final classification. The tree continues to "grow" until the maximum tree depth (that was set) is achieved. The prediction is made based on what bin an observation ends up in. 

Classification trees are easy to interpret, since the decision at each step is easy to visualize. Numeric predictors do not need to be centered or scaled. However, classification trees can become overfit by memorizing the training data and then become poor generalizers. 

### Why could we consider Classification Trees with our data?

Classification Trees should be considered with our data since we have a binary response variable. Also, we do not need to do any prepossessing of our data, since we have categorical predictors (alone with our response).

## Random Forest:

### Defining what Random Forest Models Are:

Random forest models are ensemble learning models where multiple decision trees are combined to make a prediction. During the training of the model, many trees are built and the final prediction is based on an aggregate of all of individual trees. The final predication is based on the average prediction or majority of the classification. Random Forest models are similar to classification trees, but they normally use bootstrap aggregation where each tree is training on a random subset of data. Also, at each decision point, a random subset of variables are considered (not all variables). 

The advantage of random forest models over a classification tree is that there is normally higher accuracy due to the randomness of the training data. Also, there is likely less variance and reduced overfitting. 

On the other hand, random forest models do require a high cost to compute. 

### Why could we consider Random Forests with our data?

A Random Forest Model could be considered for our data since they can predict a binary outcome. Also, the random forest model would likely create a better model than a classification tree, due to the ensemble nature of the model. 

## Comparing the 3 Models and Selecting the Best Model